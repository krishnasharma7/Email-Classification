{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-20T06:19:12.752528Z","iopub.status.busy":"2024-03-20T06:19:12.752152Z","iopub.status.idle":"2024-03-20T06:19:13.119275Z","shell.execute_reply":"2024-03-20T06:19:13.118318Z","shell.execute_reply.started":"2024-03-20T06:19:12.752483Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:19:15.311421Z","iopub.status.busy":"2024-03-20T06:19:15.310548Z","iopub.status.idle":"2024-03-20T06:19:16.750981Z","shell.execute_reply":"2024-03-20T06:19:16.750099Z","shell.execute_reply.started":"2024-03-20T06:19:15.311385Z"},"trusted":true},"outputs":[],"source":["import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import words\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:19:17.534660Z","iopub.status.busy":"2024-03-20T06:19:17.534321Z","iopub.status.idle":"2024-03-20T06:19:18.061932Z","shell.execute_reply":"2024-03-20T06:19:18.060598Z","shell.execute_reply.started":"2024-03-20T06:19:17.534632Z"},"trusted":true},"outputs":[{"ename":"SyntaxError","evalue":"(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3815282457.py, line 1)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    df = pd.read_csv('C:\\Users\\krish\\OneDrive\\Desktop\\Projects\\Email-Classification\\Dataset\\spam_assassin.csv')\u001b[0m\n\u001b[1;37m                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"]}],"source":["df = pd.read_csv('C:\\\\Users\\\\krish\\\\OneDrive\\\\Desktop\\\\Projects\\\\Email-Classification\\\\Dataset\\\\spam_assassin.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:19:19.573689Z","iopub.status.busy":"2024-03-20T06:19:19.573342Z","iopub.status.idle":"2024-03-20T06:19:19.602387Z","shell.execute_reply":"2024-03-20T06:19:19.601138Z","shell.execute_reply.started":"2024-03-20T06:19:19.573661Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>From gort44@excite.com Mon Jun 24 17:54:21 200...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>From fork-admin@xent.com Mon Jul 29 11:39:57 2...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>From dcm123@btamail.net.cn Mon Jun 24 17:49:23...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5791</th>\n","      <td>From ilug-admin@linux.ie Mon Jul 22 18:12:45 2...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5792</th>\n","      <td>From fork-admin@xent.com Mon Oct 7 20:37:02 20...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5793</th>\n","      <td>Received: from hq.pro-ns.net (localhost [127.0...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5794</th>\n","      <td>From razor-users-admin@lists.sourceforge.net T...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5795</th>\n","      <td>From rssfeeds@jmason.org Mon Sep 30 13:44:10 2...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5796 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                                   text  target\n","0     From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...       0\n","1     From gort44@excite.com Mon Jun 24 17:54:21 200...       1\n","2     From fork-admin@xent.com Mon Jul 29 11:39:57 2...       1\n","3     From dcm123@btamail.net.cn Mon Jun 24 17:49:23...       1\n","4     From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...       0\n","...                                                 ...     ...\n","5791  From ilug-admin@linux.ie Mon Jul 22 18:12:45 2...       0\n","5792  From fork-admin@xent.com Mon Oct 7 20:37:02 20...       0\n","5793  Received: from hq.pro-ns.net (localhost [127.0...       1\n","5794  From razor-users-admin@lists.sourceforge.net T...       0\n","5795  From rssfeeds@jmason.org Mon Sep 30 13:44:10 2...       0\n","\n","[5796 rows x 2 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:19:21.695789Z","iopub.status.busy":"2024-03-20T06:19:21.695447Z","iopub.status.idle":"2024-03-20T06:19:21.703659Z","shell.execute_reply":"2024-03-20T06:19:21.702323Z","shell.execute_reply.started":"2024-03-20T06:19:21.695762Z"},"trusted":true},"outputs":[],"source":["y = df['target']\n","x = df['text']"]},{"cell_type":"markdown","metadata":{},"source":["to download wordnet from NLTK"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:19:23.703327Z","iopub.status.busy":"2024-03-20T06:19:23.702949Z","iopub.status.idle":"2024-03-20T06:19:24.120035Z","shell.execute_reply":"2024-03-20T06:19:24.118739Z","shell.execute_reply.started":"2024-03-20T06:19:23.703294Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /kaggle/working/...\n","Archive:  /kaggle/working/corpora/wordnet.zip\n","   creating: /kaggle/working/corpora/wordnet/\n","  inflating: /kaggle/working/corpora/wordnet/lexnames  \n","  inflating: /kaggle/working/corpora/wordnet/data.verb  \n","  inflating: /kaggle/working/corpora/wordnet/index.adv  \n","  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n","  inflating: /kaggle/working/corpora/wordnet/index.verb  \n","  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n","  inflating: /kaggle/working/corpora/wordnet/data.adj  \n","  inflating: /kaggle/working/corpora/wordnet/index.adj  \n","  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n","  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n","  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n","  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n","  inflating: /kaggle/working/corpora/wordnet/README  \n","  inflating: /kaggle/working/corpora/wordnet/index.sense  \n","  inflating: /kaggle/working/corpora/wordnet/data.noun  \n","  inflating: /kaggle/working/corpora/wordnet/data.adv  \n","  inflating: /kaggle/working/corpora/wordnet/index.noun  \n","  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n"]}],"source":["import subprocess\n","\n","# Download and unzip wordnet\n","try:\n","    nltk.data.find('wordnet.zip')\n","except:\n","    nltk.download('wordnet', download_dir='/kaggle/working/')\n","    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n","    subprocess.run(command.split())\n","    nltk.data.path.append('/kaggle/working/')\n","\n","# Now you can import the NLTK resources as usual\n","from nltk.corpus import wordnet"]},{"cell_type":"markdown","metadata":{},"source":["Below code performs the following tasks:-\n","\n","1) create a list called emails which will extract each email from X and append it to list\n","\n","2) for each email in our dataframe X perform following tasks:-\n","\n","i) Firstly, use the re library in python to substitute the extra characters (which are not a-z or A-Z) by space and then store the new email as an item.\n","\n","ii) convert the final email into lower case and split into words based on space forming a list, then add only those words which are present in words corpus (to have only general sensible words making our text less precise maybe but cleaner)\n","\n","iii) for each word, first check if word is stopword or not. if it is not stopword then lemmatize it (convert it to its root form which makes it easier to process and also multiple variations of a word is converted to single form)\n","\n","iv) Finally, all words are now in root form. Combine them using space and then return the final email"]},{"cell_type":"markdown","metadata":{},"source":["For loop takes so long, create a function that does the processing work and then perform vectorization by converting all raw emails to a list and then calling vector function, storing output in emails list"]},{"cell_type":"markdown","metadata":{},"source":["Due to large processing time, taking sample of only 1000 emails\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:29:38.353078Z","iopub.status.busy":"2024-03-20T06:29:38.352737Z","iopub.status.idle":"2024-03-20T06:29:38.360141Z","shell.execute_reply":"2024-03-20T06:29:38.358441Z","shell.execute_reply.started":"2024-03-20T06:29:38.353049Z"},"trusted":true},"outputs":[],"source":["def process_email(email):\n","    lm = WordNetLemmatizer() #object of WordNetLemmatizer\n","    new_email = re.sub('[^a-zA-Z]',' ',str(email)) #whatever is not included in a-z and A-Z, replace with space and convert item to string\n","    new_email = new_email.lower()\n","    new_email_words = new_email.split() #convert to lower case throughout then split on space\n","    #new_email = [word for word in new_email_words if word in set(words.words())] #vectorize this operation\n","    new_email = [lm.lemmatize(word) for word in new_email_words if word not in set(stopwords.words('english'))] \n","    new_email = (' '.join(str(x) for x in new_email))\n","    return new_email"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:30:03.884197Z","iopub.status.busy":"2024-03-20T06:30:03.883858Z","iopub.status.idle":"2024-03-20T06:36:42.897070Z","shell.execute_reply":"2024-03-20T06:36:42.895944Z","shell.execute_reply.started":"2024-03-20T06:30:03.884172Z"},"trusted":true},"outputs":[],"source":["emails = [] #contains final emails after removing extra characters, stop words and performing lemmatization\n","numpy_arr_emails = np.array(x,dtype = object)\n","vect_process_email = np.vectorize(process_email,otypes = [str])\n","emails = vect_process_email(numpy_arr_emails)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:37:09.453982Z","iopub.status.busy":"2024-03-20T06:37:09.453626Z","iopub.status.idle":"2024-03-20T06:37:09.461622Z","shell.execute_reply":"2024-03-20T06:37:09.460461Z","shell.execute_reply.started":"2024-03-20T06:37:09.453957Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'ilug admin linux ie mon jul return path ilug admin linux ie delivered yyyy localhost netnoteinc com received localhost localhost phobos lab netnoteinc com postfix esmtp id f jm localhost mon jul edt received phobos localhost imap fetchmail jm localhost single drop mon jul ist received lugh tuatha org root lugh tuatha org dogma slashnull org esmtp id g rhn jm ilug jmason org sat jul received lugh root localhost lugh tuatha org esmtp id saa sat jul x authentication warning lugh tuatha org host root localhost claimed lugh received mail mail iol ie mail mail iol ie lugh tuatha org esmtp id saa ilug linux ie sat jul received dialup t cwt esat net helo hobbiton cod ie mail mail iol ie esmtp exim id yvvf w ilug linux ie sat jul received cdaly localhost hobbiton cod ie id g rdroo ilug linux ie sat jul date sat jul conor daly conor daly oceanfree net ilug main list ilug linux ie subject ilug architecture crossover trouble w rh solved message id b hobbiton cod ie mail followup ilug main list ilug linux ie reference c dce cd b c ba e fa milexc maxtor com mime version content type text plain charset u ascii content disposition inline user agent mutt reply c dce cd b c ba e fa milexc maxtor com conor wynne maxtor com fri jul pm sender ilug admin linux ie error ilug admin linux ie x mailman version precedence bulk list id irish linux user group ilug linux ie x beenthere ilug linux ie fri jul pm rumoured hereabouts wynne conor thought surely would faster save conf file install box copy back confs voila car confs boite data right yeah remember exactly confs modified etc either thats would sysadmins make life difficult complicated possible yup case two issue mirrored disk give someone else work box available p p processor celeron box crashing backup software wanted try backup different box make sure hardware related also interesting exercise thought mirroring system drive might save serious hassle line oh going africa aiming robust possible belt brace probably one jumpsuit mirroring disk worth much eg lightning strike taking disk system compromise also going backup cdr automated restore http www mondorescue org admin able build system mobo got fried replacement wrong arch compatible install mean dropping hd booting ish conor conor daly conor daly oceanfree net domestic sysadmin faenor cod ie pm day user load average hobbiton cod ie pm day user load average irish linux user group ilug linux ie http www linux ie mailman listinfo ilug un subscription information list maintainer listmaster linux ie'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["emails[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["emails = list(emails)\n","y = list(y)\n","y"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:37:37.454455Z","iopub.status.busy":"2024-03-20T06:37:37.453007Z","iopub.status.idle":"2024-03-20T06:37:37.687803Z","shell.execute_reply":"2024-03-20T06:37:37.686351Z","shell.execute_reply.started":"2024-03-20T06:37:37.454401Z"},"trusted":true},"outputs":[],"source":["processed_emails = pd.DataFrame(emails)\n","processed_emails.to_csv('processed_emails.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(emails[0])"]},{"cell_type":"markdown","metadata":{},"source":["Now, performing bag of words approach to convert text data to numbers using scikit count "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T07:33:57.156613Z","iopub.status.busy":"2024-03-17T07:33:57.156232Z","iopub.status.idle":"2024-03-17T07:33:57.176163Z","shell.execute_reply":"2024-03-17T07:33:57.174946Z","shell.execute_reply.started":"2024-03-17T07:33:57.156583Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["312\n"]}],"source":["cv = CountVectorizer(ngram_range = (1,2)) # 1,2 implies consider only bigrams at max\n","test_email = cv.fit_transform(emails).toarray() #perform bag of words then convert it to an array so that its easy to work on\n","print(len(test_email[0]))"]},{"cell_type":"markdown","metadata":{},"source":["Now, trying Term Frequency (TF) and Inverse Document Frequency (IDF) instead to compare"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:37:56.871812Z","iopub.status.busy":"2024-03-20T06:37:56.870955Z","iopub.status.idle":"2024-03-20T06:37:59.677408Z","shell.execute_reply":"2024-03-20T06:37:59.676081Z","shell.execute_reply.started":"2024-03-20T06:37:56.871781Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[0. 0. 0. ... 0. 0. 0.]\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tiv=TfidfVectorizer()\n","emails = tiv.fit_transform(emails).toarray()\n","print(emails[0])"]},{"cell_type":"markdown","metadata":{},"source":["TF-IDF approach gives lesser number (for this example atleast, check more to confirm)\n","\n","However, TF-IDF data is highly decimal intensive so normalize the data before training"]},{"cell_type":"markdown","metadata":{},"source":["Performing basic normalization using scikit"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn import preprocessing\n","test_email_2 = np.array(test_email_2)\n","test_email_2 = test_email_2.reshape(1,-1)\n","normalise_tem2 = preprocessing.normalize(test_email_2)\n","print(normalise_tem2)"]},{"cell_type":"markdown","metadata":{},"source":["This should be enough for data processing, next step process all input data, perform splitting into train, cv and test set and then build classifier model and evaluate metrics"]},{"cell_type":"markdown","metadata":{},"source":["Normalizing tf-idf array of all emails using min max scaler"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["scaler = preprocessing.MinMaxScaler()\n","test = test_email_2[0].reshape(-1,1)\n","norm = scaler.fit_transform(test)\n","print(norm)"]},{"cell_type":"markdown","metadata":{},"source":["splitting data into training and test set (66/33)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:38:11.611738Z","iopub.status.busy":"2024-03-20T06:38:11.611382Z","iopub.status.idle":"2024-03-20T06:38:13.098840Z","shell.execute_reply":"2024-03-20T06:38:13.097210Z","shell.execute_reply.started":"2024-03-20T06:38:11.611709Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","Email_train, Email_test, Y_train, Y_test = train_test_split(emails, y, test_size = 0.33, random_state = 7)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Y_test"]},{"cell_type":"markdown","metadata":{},"source":["First testing model without normalizing to see if its needed or not"]},{"cell_type":"markdown","metadata":{},"source":["For classification, following models will be built to perform binary classification and the model with most accuracy will be final:-\n","\n","1) Random Forest Classifier\n","\n","2) Decision Trees\n"," \n","3) Logistic Regression\n","\n","4) Naive Bayes Classifier"]},{"cell_type":"markdown","metadata":{},"source":["Random Forest Classifier"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:38:25.180059Z","iopub.status.busy":"2024-03-20T06:38:25.179374Z","iopub.status.idle":"2024-03-20T06:38:29.697851Z","shell.execute_reply":"2024-03-20T06:38:29.696998Z","shell.execute_reply.started":"2024-03-20T06:38:25.180023Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","clf = RandomForestClassifier(max_depth = 4,random_state = 0)\n","clf.fit(Email_train,Y_train)\n","Y_pred = clf.predict(Email_test)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:38:31.452956Z","iopub.status.busy":"2024-03-20T06:38:31.451916Z","iopub.status.idle":"2024-03-20T06:38:31.459310Z","shell.execute_reply":"2024-03-20T06:38:31.458200Z","shell.execute_reply.started":"2024-03-20T06:38:31.452909Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([1, 1, 0, ..., 1, 0, 0])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["Y_pred"]},{"cell_type":"markdown","metadata":{},"source":["Naive Bayes Classifier"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:39:58.860172Z","iopub.status.busy":"2024-03-20T06:39:58.858983Z","iopub.status.idle":"2024-03-20T06:40:05.098154Z","shell.execute_reply":"2024-03-20T06:40:05.096087Z","shell.execute_reply.started":"2024-03-20T06:39:58.860124Z"},"trusted":true},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","\n","clf = GaussianNB()\n","\n","clf.fit(Email_train,Y_train)\n","Y_pred = clf.predict(Email_test)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:40:08.810155Z","iopub.status.busy":"2024-03-20T06:40:08.808678Z","iopub.status.idle":"2024-03-20T06:40:08.823674Z","shell.execute_reply":"2024-03-20T06:40:08.822789Z","shell.execute_reply.started":"2024-03-20T06:40:08.810089Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([1, 1, 1, ..., 1, 0, 1])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["Y_pred"]},{"cell_type":"markdown","metadata":{},"source":["Accuracy Metrics - Evaluating our model performance using scikit metrics"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:42:44.675324Z","iopub.status.busy":"2024-03-20T06:42:44.674919Z","iopub.status.idle":"2024-03-20T06:42:44.692308Z","shell.execute_reply":"2024-03-20T06:42:44.691216Z","shell.execute_reply.started":"2024-03-20T06:42:44.675294Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score,precision_score,recall_score, classification_report\n","\n","acc_score = accuracy_score(Y_test,Y_pred)\n","rep = classification_report(Y_test,Y_pred)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:42:45.916857Z","iopub.status.busy":"2024-03-20T06:42:45.916221Z","iopub.status.idle":"2024-03-20T06:42:45.922296Z","shell.execute_reply":"2024-03-20T06:42:45.921602Z","shell.execute_reply.started":"2024-03-20T06:42:45.916826Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'              precision    recall  f1-score   support\\n\\n           0       0.98      0.99      0.99      1281\\n           1       0.98      0.97      0.97       632\\n\\n    accuracy                           0.98      1913\\n   macro avg       0.98      0.98      0.98      1913\\nweighted avg       0.98      0.98      0.98      1913\\n'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["rep"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T06:40:50.720830Z","iopub.status.busy":"2024-03-20T06:40:50.719999Z","iopub.status.idle":"2024-03-20T06:40:50.727225Z","shell.execute_reply":"2024-03-20T06:40:50.725688Z","shell.execute_reply.started":"2024-03-20T06:40:50.720796Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0.9811813904861474"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["acc_score"]},{"cell_type":"markdown","metadata":{},"source":["Without Cross Validation\n","\n","Random Forest Classifier : 83.64\n","\n","Naive Bayes : 98.12"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1516263,"sourceId":2503898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
